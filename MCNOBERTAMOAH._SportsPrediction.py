# -*- coding: utf-8 -*-
"""MCNOBERTAMOAH._SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aQOEoyMdbiyCoTzxWbXO6GXwxoEGTqKG
"""

# Import necessary libraries for data manipulation and analysis

import re
import joblib
import numpy as np
import pandas as pd
import xgboost as xgb
import category_encoders as ce
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import VotingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import StackingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import RandomizedSearchCV
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.ensemble import VotingRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Read the CSV files containing player data

players_22 = pd.read_csv('players_22.csv')
male_players_legacy = pd.read_csv('male_players (legacy).csv')

# A Function For Cleaning Datasets For Training
def clean_data(df,condition):

    # Drop columns containing 'url' or 'dob' in their names
    columns_to_drop = df.filter(regex='url|dob').columns
    df.drop(columns=columns_to_drop, axis=1, inplace=True)

    # Create lists to hold columns with less than 30% missing values and columns with more than 30% missing values
    L = []
    L_less = []
    for i in df.columns:
        if df[i].isnull().sum()<(0.3 * df.shape[0]):
            L.append(i)
        else:
            L_less.append(i)

    df = df[L]

    # Impute missing values: use the median for numeric columns and the most frequent value for object columns
    numeric_cols = df.select_dtypes(include=['number']).columns
    num_imputer = SimpleImputer(strategy='median')
    df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])


    obj_imputer = SimpleImputer(strategy='most_frequent')
    df[df.select_dtypes(include='object').columns] = obj_imputer.fit_transform\
        (df[df.select_dtypes
    (include='object').columns])

    if condition == "Yes":
        # Drop specified columns
        columns_to_drop = ['player_id','fifa_version','short_name', 'long_name','player_positions','club_team_id',
                           'club_name','club_joined_date','club_contract_valid_until_year','league_name',
                           'league_level','club_position','club_jersey_number','fifa_update','fifa_update_date'
                           ,'nationality_id','nationality_name','real_face']
        df.drop(columns=columns_to_drop, axis=1, inplace=True)

    # Convert specific patterns in the columns from string to numerical values
    for col in df.columns:
        # Convert column to string for checking patterns
        col_str = df[col].astype(str)

        # Check if the column contains any data in the form of "number+number" or "number-number"
        if col_str.str.contains(r'^\d+[\+\-]\d+$').any():
            # Apply eval to each element in the column if it matches the pattern
            df[col] = df[col].apply(lambda x: eval(x) if isinstance(x, str) and re.match
            (r'^\d+[\+\-]\d+$', x) else x)

            # Convert the column to int or float as needed
            if df[col].apply(lambda x: isinstance(x, (int, float))).all():
                df[col] = df[col].astype(float)

    # Encode object columns using BinaryEncoder
    for column in df.columns[:69]:
        if df[column].dtype == 'object':
            encoder = ce.BinaryEncoder(cols=[column])
            col_encoded = encoder.fit_transform(df[column])
            df.drop(column, axis=1, inplace=True)
            df = pd.concat([df, col_encoded], axis=1).reset_index(drop=True)

    return df

# Clean the first dataset

male_players_legacy = clean_data(male_players_legacy, "Yes")

# Clean the second dataset

players_22 = clean_data(players_22, "No")

# Display the DataFrame

male_players_legacy

# Calculate correlations of all features with the 'overall' column and keep the top 12 features

correlations = male_players_legacy.corr()['overall']
sorted_correlations = correlations.sort_values(ascending=False)

male_players_legacy_relevant_columns = sorted_correlations[:16]

male_players_legacy_relevant_columns.index

# Keep only the top correlated features

male_players_legacy = male_players_legacy[male_players_legacy_relevant_columns.index]

# Display the updated DataFrame

male_players_legacy

# Display the first few rows of the players_22 DataFrame

players_22

# Define target and features for model training

y = male_players_legacy['overall']
X = male_players_legacy.drop('overall', axis=1)

# Split data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the data

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert scaled data back to DataFrame for easier handling
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)

# Save the scaler for future use
joblib.dump(scaler, 'scaler.pkl')

# Display the first few rows of the training set

X_train.head()

y_train.head()

# Multi Linear Regression

# Initialize the Linear Regression model
l = LinearRegression()

# Fit the Linear Regression model on the training data

l.fit(X_train,y_train)

# Predict the target values on the test data using the Linear Regression model

y_pred = l.predict(X_test)

# Get the feature names from the training data

X_train.columns

# Get the intercept and coefficients of the trained Linear Regression model

# Get the intercept and coefficients of the trained Linear Regression model
intercept = l.intercept_
coefficients = l.coef_

# Print the intercept and coefficients of the Linear Regression model
print(f"Intercept: {intercept}")
print(f"Coefficients: {coefficients}")

# Evaluate the Linear Regression model using various metrics

print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred,y_test)},
Mean Squared Error = {mean_squared_error(y_pred,y_test)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred,y_test))},
R2 Score = {r2_score(y_pred,y_test)}
""")

# Polynomial Regression

# Create polynomial regression pipelines with degree 1 and 2
poly_pipeline1 = make_pipeline(PolynomialFeatures(degree=1), LinearRegression())
poly_pipeline2 = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())

# Transform the training and test data to include polynomial features (degree 1)
poly1 = PolynomialFeatures(degree=1)
X_poly_train = poly1.fit_transform(X_train)
X_poly_test = poly1.fit_transform(X_test)

# Initialize and fit a Linear Regression model on the polynomial transformed data
model = LinearRegression()
model.fit(X_poly_train, y_train)

# Predict the target values on the polynomial transformed test data
y_pred = model.predict(X_poly_test)

# Evaluate the Polynomial Regression model (degree 1) using various metrics

print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred, y_test)},
Mean Squared Error = {mean_squared_error(y_pred, y_test)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred, y_test))},
R2 Score = {r2_score(y_pred, y_test)}
""")

# Transform the training and test data to include polynomial features (degree 2)

poly2 = PolynomialFeatures(degree=2)
X_poly_train = poly2.fit_transform(X_train)
X_poly_test = poly2.fit_transform(X_test)

# Initialize and fit a Linear Regression model on the polynomial transformed data
model = LinearRegression()
model.fit(X_poly_train, y_train)

# Predict the target values on the polynomial transformed test data
y_pred = model.predict(X_poly_test)

# Evaluate the Polynomial Regression model (degree 2) using various metrics
print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred, y_test)},
Mean Squared Error = {mean_squared_error(y_pred, y_test)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred, y_test))},
R2 Score = {r2_score(y_pred, y_test)}
""")

# Transform the training and test data to include polynomial features (degree 3)

poly3 = PolynomialFeatures(degree=3)
X_poly_train = poly3.fit_transform(X_train)
X_poly_test = poly3.fit_transform(X_test)

# Initialize and fit a Linear Regression model on the polynomial transformed data
model = LinearRegression()
model.fit(X_poly_train, y_train)

# Predict the target values on the polynomial transformed test data
y_pred = model.predict(X_poly_test)

# Evaluate the Polynomial Regression model (degree 3) using various metrics
print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred, y_test)},
Mean Squared Error = {mean_squared_error(y_pred, y_test)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred, y_test))},
R2 Score = {r2_score(y_pred, y_test)}
""")

# Regularisation Models

# Ridge Regression

# Initialize Ridge regression mode
model1 = Ridge()

# Fit the model on the training data
model1.fit(X_train, y_train)

# Predict the target values on the test data
y_pred = model1.predict(X_test)

# Evaluate the Ridge Regression model using various metrics

print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred,y_test)},
Mean Squared Error = {mean_squared_error(y_pred,y_test)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred,y_test))},
R2 Score = {r2_score(y_pred,y_test)}
""")

# Lasso Regression

# Initialize Lasso regression model
model2 = Lasso()

# Fit the model on the training data
model2.fit(X_train, y_train)

# Predict the target values on the test data
y_pred = model2.predict(X_test)

# Evaluate the Lasso Regression model using various metrics
print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred,y_test)},
Mean Squared Error = {mean_squared_error(y_pred,y_test)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred,y_test))},
R2 Score = {r2_score(y_pred,y_test)}
""")

# ElasticNet Regression

# Initialize ElasticNet regression model
model3 = ElasticNet()

# Fit the model on the training data
model3.fit(X_train, y_train)

# Predict the target values on the test data
y_pred = model3.predict(X_test)

# Evaluate the ElasticNet Regression model using various metrics
print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred,y_test)},
Mean Squared Error = {mean_squared_error(y_pred,y_test)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred,y_test))},
R2 Score = {r2_score(y_pred,y_test)}
""")

# Decision Tree Regression

# Initialize Decision Tree Regressor with max depth of 2000
dtree = DecisionTreeRegressor(max_depth=2000)

# Fit the model on the training data

dtree.fit(X_train, y_train)

# Predict the target values on the test data
y_pred = dtree.predict(X_test)

# Evaluate the Decision Tree Regression model using various metrics
print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred,y_test)},
Mean Squared Error = {mean_squared_error(y_pred,y_test)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred,y_test))},
R2 Score = {r2_score(y_pred,y_test)}
""")

# Stacking Regressor

# Define base models for stacking
estimators = [
    ('lr', l),
    ('poly1', poly_pipeline1),
    ('poly2', poly_pipeline2),
    ('rm1', model1),
    ('rm2', model2),
    ('rm3', model3),
    ('dt', dtree)
]

# Initialize Stacking Regressor with Decision Tree as final estimator
stacking_model = StackingRegressor(estimators=estimators, final_estimator=DecisionTreeRegressor
(max_depth=10000))

# Fit the stacking model on the training data
stacking_model.fit(X_train, y_train)

# Predict the target values on the test data using the stacking model
y_pred = stacking_model.predict(X_test)

# Evaluate the Stacking Regressor model using various metrics
print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred,y_test)},
Mean Squared Error = {mean_squared_error(y_pred,y_test)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred,y_test))},
R2 Score = {r2_score(y_pred,y_test)}
""")

# Cross-validation scores for the stacking model

cv_scores = cross_val_score(stacking_model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
mean_cv_score = np.mean(cv_scores)
std_cv_score = np.std(cv_scores)


# Print the mean and standard deviation of cross-validation scores
print(f"Mean CV Score (Negative MSE): {mean_cv_score}")
print(f"Standard Deviation of CV Scores: {std_cv_score}")

# Grid search for hyperparameter tuning of the final estimator in the stacking model

param_grid = {
    'final_estimator__max_depth': [5, 10, 15, 20],
    'final_estimator__min_samples_split': [2, 5, 10]
}


grid_search = GridSearchCV(stacking_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit the grid search on the training data
grid_search.fit(X_train_scaled, y_train)

# Best estimator from the grid search
test_model_1 = grid_search.best_estimator_

# Predict the target values on the test data using the best estimator
y_pred_best = test_model_1.predict(X_test_scaled)

# Evaluate the best estimator model using various metrics
print(f"Test Model 1 Mean Absolute Error: {mean_absolute_error(y_test, y_pred_best)}")
print(f"Test Model 1 Mean Squared Error: {mean_squared_error(y_test, y_pred_best)}")
print(f"Test Model 1 Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred_best))}")
print(f"Test Model 1 R2 Score: {r2_score(y_test, y_pred_best)}")

# Refined grid search for hyperparameter tuning of the best estimator

refined_param_grid = {
    'final_estimator__max_depth': [12, 15, 18],
    'final_estimator__min_samples_split': [5, 7, 10]
}

refined_grid_search = GridSearchCV(test_model_1, refined_param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit the refined grid search on the training data
refined_grid_search.fit(X_train_scaled, y_train)

# Best estimator from the refined grid search
test_model_2 = refined_grid_search.best_estimator_

# Fit the model on the training data
test_model_2.fit(X_train_scaled, y_train)

# Predict the target values on the test data using the model
y_pred_final = test_model_2.predict(X_test_scaled)

# Evaluate the model using various metrics
print(f"Test Model 2 Mean Absolute Error: {mean_absolute_error(y_test, y_pred_final)}")
print(f"Test Model 2 Mean Squared Error: {mean_squared_error(y_test, y_pred_final)}")
print(f"Test Model 2 Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred_final))}")
print(f"Test Model 2 R2 Score: {r2_score(y_test, y_pred_final)}")

# Define refined parameter grid for hyperparameter tuning of the final estimator in the stacking model

refined_param_grid = {
    'final_estimator__max_depth': [10, 15],
    'final_estimator__min_samples_split': [5, 10]
}

# Perform grid search with cross-validation to find the best hyperparameters for the final estimator
refined_grid_search = GridSearchCV(test_model_2, refined_param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)

# Fit the grid search on the training data
refined_grid_search.fit(X_train_scaled, y_train)

# Get the best estimator from the grid search
test_model_3 = refined_grid_search.best_estimator_

# Fit the best estimator on the training data
test_model_3.fit(X_train_scaled, y_train)

# Predict the target values on the test data using the best estimator
y_pred_final = test_model_3.predict(X_test_scaled)

# Evaluate the best estimator model using various metrics
print(f"Test Model 3 Mean Absolute Error: {mean_absolute_error(y_test, y_pred_final)}")
print(f"Test Model 3 Mean Squared Error: {mean_squared_error(y_test, y_pred_final)}")
print(f"Test Model 3 Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred_final))}")
print(f"Test Model 3 R2 Score: {r2_score(y_test, y_pred_final)}")

# Initialize and fit XGBoost Regressor with specified parameters

xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, max_depth=6, learning_rate=0.1)

# Fit the model
xgb_model.fit(X_train_scaled, y_train)

# Predict the target values on the test data using XGBoost
y_pred_xgb = xgb_model.predict(X_test_scaled)

# Evaluate the XGBoost model using various metrics
print(f"XGBoost Mean Absolute Error: {mean_absolute_error(y_test, y_pred_xgb)}")
print(f"XGBoost Mean Squared Error: {mean_squared_error(y_test, y_pred_xgb)}")
print(f"XGBoost Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred_xgb))}")
print(f"XGBoost R2 Score: {r2_score(y_test, y_pred_xgb)}")

# Initialize and fit AdaBoost Regressor with specified parameters

ada_model = AdaBoostRegressor(n_estimators=100, learning_rate=0.1)


ada_model.fit(X_train_scaled, y_train)

# Predict the target values on the test data using AdaBoost
y_pred_ada = ada_model.predict(X_test_scaled)

# Evaluate the AdaBoost model using various metrics
print(f"AdaBoost Mean Absolute Error: {mean_absolute_error(y_test, y_pred_ada)}")
print(f"AdaBoost Mean Squared Error: {mean_squared_error(y_test, y_pred_ada)}")
print(f"AdaBoost Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred_ada))}")
print(f"AdaBoost R2 Score: {r2_score(y_test, y_pred_ada)}")

# Define base models for ensemble
models = [
    ('xgb', xgb_model),
    ('dt', DecisionTreeRegressor()),
    ('tmodel1', test_model_1),
    ('tmodel3', test_model_3),
    ('adaboost',ada_model)
]

# Initialize and fit Voting Regressor as ensemble model with base models
ensemble_model = VotingRegressor(estimators=models)

# Fit
ensemble_model.fit(X_train_scaled, y_train)

# Predict the target values on the test data using the ensemble model
y_pred_ensemble = ensemble_model.predict(X_test_scaled)

# Evaluate the ensemble model using various metrics
print(f"Ensemble Model Mean Absolute Error: {mean_absolute_error(y_test, y_pred_ensemble)}")
print(f"Ensemble Model Mean Squared Error: {mean_squared_error(y_test, y_pred_ensemble)}")
print(f"Ensemble Model Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred_ensemble))}")
print(f"Ensemble Model R2 Score: {r2_score(y_test, y_pred_ensemble)}")

# Define parameter grid for Decision Tree Regressor

param_grid_dt = {
    'max_depth': [3, 5, 7, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Perform grid search with cross-validation to find the best hyperparameters for Decision Tree Regressor
grid_search_dt = GridSearchCV(estimator=DecisionTreeRegressor(), param_grid=param_grid_dt, cv=5)
grid_search_dt.fit(X_train_scaled, y_train)

# Get the best estimator from the grid search for Decision Tree Regressor
best_dt = grid_search_dt.best_estimator_

# Define parameter grid for AdaBoost Regressor
param_grid_ada = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1]
}

# Perform grid search with cross-validation to find the best hyperparameters for AdaBoost Regressor
grid_search_ada = GridSearchCV(estimator=AdaBoostRegressor(), param_grid=param_grid_ada, cv=5)
grid_search_ada.fit(X_train_scaled, y_train)

# Get the best estimator from the grid search for AdaBoost Regressor
best_ada = grid_search_ada.best_estimator_

# Update the models list with the best estimators
models = [
    ('xgb', xgb_model),
    ('dt', best_dt),
    ('tmodel1', test_model_1),
    ('tmodel3', test_model_3),
    ('adaboost', best_ada)
]

# Ensemble model with weights
weights = [1, 1, 1, 1, 1]
ensemble_model = VotingRegressor(estimators=models, weights=weights)

# Fit the ensemble model on the training data and predict on the test data
ensemble_model.fit(X_train_scaled, y_train)
y_pred_ensemble = ensemble_model.predict(X_test_scaled)

# Evaluate the ensemble model using various metrics
print(f"Ensemble Model Mean Absolute Error: {mean_absolute_error(y_test, y_pred_ensemble)}")
print(f"Ensemble Model Mean Squared Error: {mean_squared_error(y_test, y_pred_ensemble)}")
print(f"Ensemble Model Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred_ensemble))}")
print(f"Ensemble Model R2 Score: {r2_score(y_test, y_pred_ensemble)}")

# Define the parameter grid for Randomized Search on XGBoost model

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [4, 6, 8],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Perform randomized search with cross-validation for hyperparameter tuning of XGBoost model
random_search = RandomizedSearchCV(xgb_model, param_distributions=param_grid, n_iter=50, cv=5, scoring='neg_mean_absolute_error',
                                   n_jobs=-1, random_state=42)
random_search.fit(X_train_scaled, y_train)

# Get the best model from the randomized search
best_model = random_search.best_estimator_

# Predict the target values on the test data using the best model
y_pred_best = best_model.predict(X_test_scaled)

# Evaluate the best model using various metrics
print(f"Tuned Ensemble Model Mean Absolute Error: {mean_absolute_error(y_test, y_pred_best)}")
print(f"Tuned Ensemble Model Mean Squared Error: {mean_squared_error(y_test, y_pred_best)}")
print(f"Tuned Ensemble Model Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred_best))}")
print(f"Tuned Ensemble Model R2 Score: {r2_score(y_test, y_pred_best)}")

# Update the models list with the best models

models = [
    ('xgb', xgb_model),
    ('dt', DecisionTreeRegressor()),
    ('adaboost',ada_model),
    ('bm', best_model),
    ('bm2', ensemble_model)
]

# Initialize and fit a new Voting Regressor ensemble model with the updated models list
ensemble_model = VotingRegressor(estimators=models)
ensemble_model.fit(X_train_scaled, y_train)

# Predict the target values on the test data using the new ensemble model
y_pred_ensemble = ensemble_model.predict(X_test_scaled)

# Evaluate the new ensemble model using various metrics
print(f"Ensemble Model Mean Absolute Error: {mean_absolute_error(y_test, y_pred_ensemble)}")
print(f"Ensemble Model Mean Squared Error: {mean_squared_error(y_test, y_pred_ensemble)}")
print(f"Ensemble Model Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred_ensemble))}")
print(f"Ensemble Model R2 Score: {r2_score(y_test, y_pred_ensemble)}")

players_22 = male_players_legacy[male_players_legacy_relevant_columns.index]

# Split the testing data into new training and testing sets for another evaluation

y1 = players_22['overall']
x1 = players_22.drop('overall', axis=1)
X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(x1, y1, test_size=0.2, random_state=42)

# Scale the new training and testing sets
X_train_new_scaled = scaler.fit_transform(X_train_new)
X_test_new_scaled = scaler.transform(X_test_new)


# Predict the target values on the new test data using the ensemble model
tester = ensemble_model.predict(X_test_new_scaled)

# Evaluate the ensemble model on the new test data using various metrics
print(f"Tuned Ensemble Model Mean Absolute Error: {mean_absolute_error(y_test_new, tester)}")
print(f"Tuned Ensemble Model Mean Squared Error: {mean_squared_error(y_test_new, tester)}")
print(f"Tuned Ensemble Model Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test_new, tester))}")
print(f"Tuned Ensemble Model R2 Score: {r2_score(y_test_new, tester)}")

# Evaluate the ensemble model on the new test data using various metrics
joblib.dump(ensemble_model, 'stacking_model.pkl', compress=3)

players_22